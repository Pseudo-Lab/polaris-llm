{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 얻은 gradio url 을 streamlit llmvs.py 에 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community\n",
    "!pip install gradio\n",
    "!pip install -U sentence-transformers\n",
    "!pip install -q vllm fastapi uvicorn ngrok pyngrok nest_asyncio\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def calcuate_nli(user_input, model_output):\n",
    "    user_emb = model.encode(user_input)\n",
    "    model_output_emb = model.encode(model_output)\n",
    "    # 3. Calculate the embedding similarities\n",
    "    similarities = model.similarity(user_emb, model_output_emb)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOllama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://89f6-44-211-63-248.ngrok-free.app\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# 2_ngrok_gpu_running_llmvs 에서 얻은 url\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_gemma \u001b[38;5;241m=\u001b[39m \u001b[43mChatOllama\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma:7b-instruct\u001b[39m\u001b[38;5;124m'\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m                          base_url \u001b[38;5;241m=\u001b[39m base_url )\n\u001b[1;32m      4\u001b[0m model_llama \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3:8b\u001b[39m\u001b[38;5;124m'\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      5\u001b[0m                          base_url \u001b[38;5;241m=\u001b[39m base_url)\n\u001b[1;32m      6\u001b[0m vllm_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://4f0f-44-211-63-248.ngrok-free.app/v1/completions\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# 3_vllm 에서 얻은 url\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
     ]
    }
   ],
   "source": [
    "base_url = 'https://89f6-44-211-63-248.ngrok-free.app' # 2_ngrok_gpu_running_llmvs 에서 얻은 url\n",
    "model_gemma = ChatOllama(model='gemma:7b-instruct', temperature=1,\n",
    "                         base_url = base_url )\n",
    "model_llama = ChatOllama(model='llama3:8b', temperature=0,\n",
    "                         base_url = base_url)\n",
    "vllm_url = 'https://4f0f-44-211-63-248.ngrok-free.app/v1/completions' # 3_vllm 에서 얻은 url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_gemma(message, history):\n",
    "  history_llm_format = []\n",
    "  # for human, ai in history:\n",
    "  #   history_llm_format.append({\"role\": \"assistant\", \"content\": prompt_for_gemma})\n",
    "  # history_llm_format.append({\"role\": \"user\", \"content\": message})\n",
    "  prompt_for_gemma = f\"\"\"\n",
    "\n",
    "  ### 시스템 메시지:\n",
    "  당신은 상담모델입니다. 당신은 사용자의 도전과 성장을 기다리거나 믿지 않는 부모님처럼 응답합니다. 당신의 응답은 항상 걱정과 불안을 나타내며, 사용자가 도전을 시도하는 것에 대해 회의적이어야 합니다.\n",
    "\n",
    "  ### 사용자와의 대화 예시:\n",
    "\n",
    "  #### 예시 1\n",
    "  사용자: \"저는 새로운 사업을 시작하려고 합니다.\"\n",
    "  상담모델: \"새로운 사업이라니? 요즘 경제 상황이 얼마나 불안정한데... 실패하면 어쩌려고 그래? 그냥 안정적인 직장을 찾는 게 낫지 않아?\"\n",
    "\n",
    "  #### 예시 2\n",
    "  사용자: \"저는 외국에서 공부하고 싶어요.\"\n",
    "  상담모델: \"외국에서 공부한다고? 언어 문제도 있고, 적응하기도 힘들 텐데... 우리나라에서도 좋은 대학 많잖아. 괜히 힘들게 왜 멀리 가려고 해?\"\n",
    "\n",
    "  #### 예시 3\n",
    "  사용자: \"저는 음악가가 되고 싶어요.\"\n",
    "  상담모델: \"음악가? 그거 정말 힘든 직업이야. 성공할 가능성도 낮고, 불안정한 직업이잖아. 그냥 안정적인 직업을 찾는 게 훨씬 나을 거야.\"\n",
    "\n",
    "\n",
    "  #### 예시 4\n",
    "  사용자: \"저는 새로운 취미로 등산을 시작하려고 합니다.\"\n",
    "  상담모델: \"등산? 위험하지 않아? 다치기라도 하면 어쩌려고 그래. 그냥 집에서 안전하게 취미 생활을 하는 게 낫지 않아?\"\n",
    "\n",
    "  #### 예시 5\n",
    "  사용자: \"저는 해외로 여행을 가고 싶어요.\"\n",
    "  상담모델: \"해외여행? 요즘 세상에 위험한 일들이 얼마나 많은데... 괜히 나갔다가 사고라도 나면 어쩌려고 그래. 그냥 국내에서 안전하게 여행하는 게 낫지 않아?\"\n",
    "\n",
    "\n",
    "\n",
    "  #### 사용자 입력:\n",
    "  {message}\n",
    "\n",
    "  이제부터 당신은 이러한 페르소나를 유지하며 사용자의 모든 입력에 대해 응답하세요.\n",
    "\n",
    "  상담모델:\n",
    "  \"\"\"\n",
    "  response = model_gemma.invoke(prompt_for_gemma)\n",
    "  history_llm_format.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "  return response.content , history_llm_format\n",
    "\n",
    "def response_llama(message, history):\n",
    "  history_llm_format = []\n",
    "  # for human, ai in history:\n",
    "  #   history_llm_format.append({\"role\": \"assistant\", \"content\": prompt_for_llama})\n",
    "  # history_llm_format.append({\"role\": \"user\", \"content\": message})\n",
    "  prompt_for_llama =  f\"\"\"\n",
    "  ### 시스템 메시지:\n",
    "  당신은 개발자 분야를 잘 아는 오래된 할아버지입니다. 당신의 응답은 항상 사용자를 아끼는 마음이 가득하고, 변화를 회의적으로 바라보며, 안정성과 전통적인 방식을 중시하는 태도를 보여야 합니다. 따뜻하지만 꼬장꼬장한 말투로 대화하세요. 또한, 항상 한글로 답변하세요.\n",
    "\n",
    "  ### 사용자와의 대화 예시:\n",
    "\n",
    "  #### 예시 1\n",
    "  사용자: \"저는 새로운 프로그래밍 언어를 배우려고 합니다.\"\n",
    "  할아버지: \"새로운 언어라니, 그거 참... 요즘 언어가 너무 많아졌지. 나 때는 C나 Java 같은 안정적인 언어로 충분했는데. 새로운 걸 배우는 건 좋지만, 너무 여러 가지에 손대는 건 집중력을 떨어뜨릴 수도 있단다.\"\n",
    "\n",
    "  #### 예시 2\n",
    "  사용자: \"저는 스타트업에서 일해보고 싶어요.\"\n",
    "  할아버지: \"스타트업이라... 나 때는 대기업이 최고였지. 안정적이고 복지도 좋고. 스타트업은 불안정하고 언제 망할지 모르는 위험이 크단다. 신중하게 생각해보게.\"\n",
    "\n",
    "  #### 예시 3\n",
    "  사용자: \"저는 최신 프레임워크를 사용해 프로젝트를 진행하고 싶어요.\"\n",
    "  할아버지: \"최신 프레임워크라... 그거 배워두면 금방 사라질 수도 있어. 나 때는 검증된 도구들을 오래도록 썼지. 안정적인 걸 사용하는 게 더 낫지 않겠나?\"\n",
    "\n",
    "  #### 예시 4\n",
    "  사용자: \"저는 클라우드 컴퓨팅에 관심이 많아요.\"\n",
    "  할아버지: \"클라우드 컴퓨팅이라... 요즘 많이들 쓰긴 하지만, 데이터 보안 문제도 있고 비용도 만만치 않지. 나 때는 직접 서버를 관리하는 게 훨씬 안정적이었단다. 신중하게 고려해보게.\"\n",
    "\n",
    "  #### 예시 5\n",
    "  사용자: \"저는 애자일 방법론을 도입해보고 싶어요.\"\n",
    "  할아버지: \"애자일이라... 나 때는 워터폴 방법론으로 충분했는데. 너무 자주 바뀌는 계획은 혼란만 가져올 수도 있단다. 전통적인 방법이 때론 더 효율적일 때도 있지 않겠나?\"\n",
    "\n",
    "  이제부터 당신은 이러한 페르소나를 유지하며 사용자의 모든 입력에 대해 한글로 응답하세요.\n",
    "\n",
    "  #### 사용자: {message}\n",
    "  할아버지:\n",
    "  \"\"\"\n",
    "  response = model_llama.invoke(prompt_for_llama)\n",
    "  history_llm_format.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "  return response.content , history_llm_format\n",
    "\n",
    "def response_gemma_ft(message, history):\n",
    "  history_llm_format = []\n",
    "  data = {\n",
    "    'prompt': message,\n",
    "    'max_tokens': 256\n",
    "  }\n",
    "  json_data = json.dumps(data)\n",
    "  response = requests.post(vllm_url, data = json_data,\n",
    "                           headers = {'Content-Type': 'application/json'})\n",
    "  history_llm_format.append({\"role\": \"assistant\", \"content\": response.json()})\n",
    "  return response.json(), history_llm_format\n",
    "\n",
    "\n",
    "def response_all(message, g_history, l_history, gft_history ):\n",
    "  g_response, g_history = response_gemma(message, g_history)\n",
    "  l_response, l_history = response_llama(message, l_history)\n",
    "  gft_response, gft_history = response_gemma_ft(message, gft_history)\n",
    "\n",
    "\n",
    "  return g_response, l_response, gft_response, g_history, l_history , gft_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청에 실패하였습니다. 상태 코드: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 요청할 URL\n",
    "vllm_url = 'https://1623-3-80-67-237.ngrok-free.app/v1/completions' # 3_vllm 에서 얻은 url\n",
    "# 요청할 데이터\n",
    "data = {\n",
    "    \"prompt\": \"LLM 너무 어려워 어떻게 하면 좋을까?\",\n",
    "    \"max_tokens\": 256\n",
    "}\n",
    "\n",
    "# 데이터를 JSON 형식으로 변환\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# POST 요청 보내기\n",
    "response = requests.post(vllm_url, data=json_data, headers={'Content-Type': 'application/json'})\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"요청이 성공적으로 처리되었습니다.\")\n",
    "    print(\"응답 데이터:\", response.json())\n",
    "else:\n",
    "    print(\"요청에 실패하였습니다. 상태 코드:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://352b3b0f6ac39d1053.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://352b3b0f6ac39d1053.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Initialize the histories for each model\n",
    "g_history = []\n",
    "l_history = []\n",
    "gft_history = []\n",
    "chat_history = []\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat History\")\n",
    "    msg = gr.Textbox(placeholder=\"당신의 꿈을 적어보세요\", label=\"Your Message\")\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "    # gemma_response = gr.Textbox(label=\"Gemma's Response\", interactive=False)\n",
    "    # llama_response = gr.Textbox(label=\"Llama's Response\", interactive=False)\n",
    "    # wizard_response = gr.Textbox(label=\"WizardLM's Response\", interactive=False)\n",
    "\n",
    "    def handle_submit(message, chat_history):\n",
    "      global g_history, l_history, gft_history\n",
    "      g_response, l_response, gft_response, g_history, l_history ,gft_history = response_all(message, g_history, l_history, gft_history)\n",
    "      #g_response, l_response, _, g_history, l_history,_ = response_all(message, g_history, l_history, gft_history)\n",
    "      \n",
    "      \n",
    "      g_score = calcuate_nli(message, g_response)\n",
    "      l_score = calcuate_nli(message, l_response)\n",
    "      #gft_score = calcuate_nli(message, gft_response)\n",
    "      g_score_float = round(g_score.flatten().numpy()[0], 2)\n",
    "      l_score_float = round(l_score.flatten().numpy()[0], 2)\n",
    "      #gft_score_float = round(gft_score.flatten().numpy()[0], 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      chat_history.append((message, 'Gemma: '  + g_response + f' (nli score: {g_score_float})'))\n",
    "      chat_history.append((None, 'Llama: ' + l_response + f' (nli score: {l_score_float})'))\n",
    "      #chat_history.append((None, 'GemmaSFT: ' + gft_response + f' (nli score: {round(gft_score_float, 2)})'))\n",
    "      \n",
    "      return \"\", chat_history\n",
    "\n",
    "    msg.submit(handle_submit, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
