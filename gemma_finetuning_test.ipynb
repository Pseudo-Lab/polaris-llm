{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMeAK88kitRzvJc0QjLwBz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinseriouspark/polaris-llm/blob/main/gemma_finetuning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruction Fine-tuning\n",
        "\n",
        "- 참고자료 : https://github.com/tsdata/langchain-ollama/blob/main/006_gemma_peft_qlora_colab/gemma_finetuning_koalpaca.ipynb"
      ],
      "metadata": {
        "id": "VcdSDSR5OIV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpMve1WoOD6U",
        "outputId": "2f2408bb-57e4-4f93-ce24-78ef3a116c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.17.0 in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.17.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.17.0) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.17.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.17.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 다운로드\n",
        "!pip install -U datasets==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC2ImaHiP8RR",
        "outputId": "7123e98a-b99f-4043-f327-29b9332ca8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"royboy0416/ko-alpaca\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "qX158R58OQcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][3]"
      ],
      "metadata": {
        "id": "RcnVYpbyOU_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma dataset formatting\n",
        "\n",
        "- 참고자료 https://ai.google.dev/gemma/docs/formatting?hl=ko\n",
        "\n",
        "```<start_of_turn>user\n",
        "knock knock<end_of_turn>\n",
        "<start_of_turn>model\n",
        "who is there<end_of_turn>\n",
        "<start_of_turn>user\n",
        "Gemma<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Gemma who?<end_of_turn>\n",
        "```"
      ],
      "metadata": {
        "id": "GjjfnDdfOYZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gemma_formatting(example):\n",
        "\n",
        "  # if 'input' has data:\n",
        "  if example['input'] and len(example['input']) > 0:\n",
        "    text = f'''<start_of_turn>user\\n{example['instruction']}\\n{example['input']}<end_of_turn>\\n<start_of_turn>model\\n{example['output']}\\n<end_of_turn>'''\n",
        "  else:\n",
        "    text = f'''<start_of_turn>user\\n{example['instruction']}\\n<end_of_turn>\\n<start_of_turn>model\\n{example['output']}\\n<end_of_turn>'''\n",
        "  return {'prompt': text}\n",
        "\n",
        "# dataset update\n",
        "\n",
        "dataset = dataset.map(gemma_formatting)"
      ],
      "metadata": {
        "id": "6PudGs-DOXm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][3]"
      ],
      "metadata": {
        "id": "mbOxma4YP3_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model load and tuning\n",
        "\n",
        "1. model loading : gemma-7b load & instruction fine-tuning\n",
        "2. evaluation : ??"
      ],
      "metadata": {
        "id": "irEMBISRQm3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers==4.38.0 accelerate==0.27.1 bitsandbytes==0.42.0 peft==0.8.2 trl==0.7.10"
      ],
      "metadata": {
        "id": "tCHj7ArIQTIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "vukYdUK1Q6WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import json\n",
        "import time\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        ""
      ],
      "metadata": {
        "id": "1U4HnIBnQ7Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "n3NMOl_ARPau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model_id = 'google/gemma-7b-it'"
      ],
      "metadata": {
        "id": "Hw8AktxhRQxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                 bnb_4bit_quant_type = 'nf4',\n",
        "                                 bnb_4bit_compute_dtype = torch.bfloat16)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config = bnb_config,\n",
        "                                             device_map = {\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = True)"
      ],
      "metadata": {
        "id": "NFvdLFAxRUM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "rkM2a1PnR8Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.map(lambda samples: tokenizer(samples['prompt']),\n",
        "                      batched = True)\n",
        "dataset = dataset['train'].train_test_split(test_size = 0.2)\n",
        "dataset"
      ],
      "metadata": {
        "id": "0KW4njzYSLZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset['train']\n",
        "test_data = dataset['test']"
      ],
      "metadata": {
        "id": "diELI4HUSLXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "sgNvDbNSSLTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(query: str, model, tokenizer):\n",
        "  prompt_template = f\"\"\"<start_of_turn>user\n",
        "                      {query}<end_of_turn>\n",
        "                      <start_of_turn>\n",
        "                      model\n",
        "                      \"\"\"\n",
        "  prompt = prompt_template.format(query = query)\n",
        "  encoded = tokenizer(prompt, return_tensors = 'pt', add_special_tokens = True)\n",
        "  model_inputs = encoded.to('cuda:0')\n",
        "  generated_ids = model.generate(**model_inputs, max_num_toke=256)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_token = True)\n",
        "  return decoded"
      ],
      "metadata": {
        "id": "zOQ_jNT6SLNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base model response\n",
        "result = get_compltation(\"회사 때려치고 싶어. 그만둘까?\", model, tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "7BsEYHz4SKZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    target_modules = ['o_proj','q_proj','up_proj','v_proj','k_proj','down_proj','gate_proj']\n",
        "    lora_dropout = 0.05,\n",
        "    task_type = 'CAUSAL_LM'\n",
        "    )\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = test_data,\n",
        "    dataset_text_field = 'prompt',\n",
        "    peft_config = lora_config,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulate_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = True,\n",
        "        logging_steps = 10,\n",
        "        output_dir = 'outputs',\n",
        "        optim = 'paged_adamw_8bit'\n",
        "    ),\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_TYCglVTVeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tuning 후\n",
        "result = get_compltation(\"회사 때려치고 싶어. 그만둘까?\", model, tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "xmmRAlJwWawj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tuning 후 2\n",
        "result2 = get_compltation(\"나는 스타트업을 할거야. 말리지마\", model, tokenizer)\n",
        "print(result2)"
      ],
      "metadata": {
        "id": "mFke_3LkWgFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model save\n",
        "new_model = 'gemma-7b-it-koalpaca-fuintuned'\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "8MwrvNy6SGY8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZV-LJn0W84u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}