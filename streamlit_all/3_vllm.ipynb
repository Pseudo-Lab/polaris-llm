{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 실행 후 마지막 ngrok url을 4_multi-llm-gradio.ipynb에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
      "Hit:1 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease\n",
      "Hit:2 https://download.docker.com/linux/ubuntu focal InRelease                 \u001b[0m\n",
      "Hit:3 https://cli.github.com/packages stable InRelease                         \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease                         \u001b[0m\n",
      "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease               \u001b[0m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \u001b[0m\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease               \u001b[0m\n",
      "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease3m\n",
      "Hit:9 https://ngrok-agent.s3.amazonaws.com buster InRelease                    \u001b[33m\n",
      "Reading package lists... Done\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "13 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libgles2 libopengl0\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following packages will be REMOVED:\n",
      "  libnvidia-gl-535\n",
      "The following NEW packages will be installed:\n",
      "  ngrok\n",
      "0 upgraded, 1 newly installed, 1 to remove and 13 not upgraded.\n",
      "1 not fully installed or removed.\n",
      "Need to get 6505 kB of archives.\n",
      "After this operation, 463 MB disk space will be freed.\n",
      "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.11.0 [6505 kB]\n",
      "Fetched 6505 kB in 1s (7937 kB/s)\u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "(Reading database ... 58492 files and directories currently installed.).................................................] \u001b8(Reading database ... \n",
      "Removing libnvidia-gl-535:amd64 (535.183.01-0ubuntu1) ...\n",
      "\u001b[1mdpkg:\u001b[0m error processing package libnvidia-gl-535:amd64 (--remove):\n",
      " unable to securely remove '/usr/lib/x86_64-linux-gnu/libnvoptix.so.535.183.01': Device or resource busy\n",
      "\u001b[1mdpkg:\u001b[0m too many errors, stopping\n",
      "Errors were encountered while processing:\n",
      " libnvidia-gl-535:amd64\n",
      "Processing was halted because there were too many errors.\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J\u001b[1;31mE: \u001b[0mSub-process /usr/bin/dpkg returned an error code (1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\\n",
    "\t| sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null \\\n",
    "\t&& echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n",
    "\t| sudo tee /etc/apt/sources.list.d/ngrok.list \\\n",
    "\t&& sudo apt update \\\n",
    "\t&& sudo apt install ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vllm fastapi uvicorn ngrok pyngrok nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from vllm import LLM, SamplingParams\n",
    "import uvicorn\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "from starlette.responses import StreamingResponse\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34cd92cf05a48d8886ac2b998724236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrok.set_auth_token('2hgvs8djD2xuTeO4svNzxiIM7G4_7qyy1Jo94tfQ3Acvoba1t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-15 04:21:10 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='bradmin/gemma-2b-persona-instruct', speculative_config=None, tokenizer='bradmin/gemma-2b-persona-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=bradmin/gemma-2b-persona-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-15 04:21:11 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-15 04:21:11 selector.py:51] Using XFormers backend.\n",
      "INFO 06-15 04:21:12 selector.py:131] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-15 04:21:12 selector.py:51] Using XFormers backend.\n",
      "WARNING 06-15 04:21:12 gemma.py:54] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "INFO 06-15 04:21:12 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 06-15 04:21:14 model_runner.py:160] Loading model weights took 4.6720 GB\n",
      "INFO 06-15 04:21:16 gpu_executor.py:83] # GPU blocks: 23501, # CPU blocks: 14563\n",
      "INFO 06-15 04:21:18 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-15 04:21:18 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-15 04:21:28 model_runner.py:965] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "FINTUNED_MODEL = \"bradmin/gemma-2b-persona-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tensor_parallel_size = int(os.environ.get(\"DEVICES\", \"1\"))\n",
    "\n",
    "llm = LLM(\n",
    "    model=FINTUNED_MODEL,\n",
    "    dtype=torch.float16,\n",
    "    # kv_cache_dtype=\"fp8\",\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2024-06-15T04:21:28+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
      "INFO:     Started server process [10267]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://4f0f-44-211-63-248.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "formatted_prompt = \"\"\"<bos><start_of_turn>user\n",
    "당신은 유저의 대답을 듣고 그를 진정시킨 후, 그의 이야기가 얼마나 현실성 없는 이야기인지 대답해주는 모델입니다. 다양한 방식으로 그를 설득해 회사에 남아있고 그의 목표를 나중에 시도하라고 이야기 해주세요.\n",
    "\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "\"\"\"\n",
    "\n",
    "class CompletionRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int\n",
    "    min_tokens: int\n",
    "    repetition_penalty: float\n",
    "    temperature: float\n",
    "    top_k: int\n",
    "    top_p: float\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "async def completions(request: CompletionRequest):\n",
    "  sampling_params = SamplingParams(\n",
    "      max_tokens=request.max_tokens,\n",
    "      min_tokens=request.min_tokens,\n",
    "      repetition_penalty=request.repetition_penalty,\n",
    "      temperature=request.temperature,\n",
    "      top_k=request.top_k,\n",
    "      top_p=request.top_p,\n",
    "      )\n",
    "  prompt = formatted_prompt.format(request.prompt)\n",
    "  print(prompt)\n",
    "  response = llm.generate([prompt], sampling_params)\n",
    "  ans = response[0].outputs[0].text\n",
    "  return ans\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"Public URL: {public_url}\")\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
